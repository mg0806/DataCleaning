# -*- coding: utf-8 -*-
"""210410107067-Manohar-Gupta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LPQiidQZrlz-2-DgvMVKe4_vF0Gh9C8U

# Supermarket Sales Data Analysis and Preprocessing
This notebook focuses on cleaning and preprocessing supermarket sales data. The steps include handling missing values, normalizing data, feature scaling, dimensionality reduction using PCA, and feature selection.
"""

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from google.colab import files

# File upload
uploaded = files.upload()

# Load dataset (assuming the uploaded file is an Excel file)
for file_name in uploaded.keys():
    df = pd.read_excel(file_name)

# Initial data exploration
print("Dataset Overview:")
df.head()  # View first few rows
print("\nData Info:")
df.info() # Information about the dataset
print("\nSummary Statistics:")
df.describe()  # Summary statistics

# Check for missing values
print("\nMissing Values:")
df.isnull().sum()

"""- The dataset is loaded, and basic exploratory data analysis (EDA) is done with head(), info(), describe(), and checking for missing values."""

from sklearn.impute import SimpleImputer

# Replace '?' with NaN
df.replace('?', np.nan, inplace=True)

# Impute missing values in numeric columns with the median
numeric_columns = df.select_dtypes(include=[np.number]).columns
imputer_median = SimpleImputer(strategy='median')
df[numeric_columns] = imputer_median.fit_transform(df[numeric_columns])

# Impute missing values in non-numeric columns with the most frequent value (mode)
categorical_columns = df.select_dtypes(exclude=[np.number]).columns
imputer_mode = SimpleImputer(strategy='most_frequent')
df[categorical_columns] = imputer_mode.fit_transform(df[categorical_columns])

# Check for missing values after imputation
print("Missing Values After Imputation:")
df.isnull().sum()

# Visualize outliers using a boxplot
plt.figure(figsize=(15, 10))
sns.boxplot(data=df[numeric_columns])
plt.title('Boxplot of Numeric Columns')
plt.show()

"""- This section handles missing values using different strategies for numeric (median) and categorical (mode) columns. Then, it visualizes outliers using a boxplot."""

from sklearn.preprocessing import StandardScaler

# Feature scaling for numeric columns
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[numeric_columns])

# Converting scaled data back to DataFrame
df_scaled = pd.DataFrame(scaled_data, columns=numeric_columns)

# Display the scaled data summary
print("Scaled Data Summary:")
df_scaled.describe()

"""- This section applies standard scaling to numeric columns to normalize the dataset.
- If new features are needed, you can add more here, though itâ€™s not a strict requirement in this case.
"""

from sklearn.decomposition import PCA

# Apply PCA for dimensionality reduction (adjust the number of components based on your dataset)
pca = PCA(n_components=2)
pca_data = pca.fit_transform(df_scaled)

# Explained variance of the components
explained_variance = pca.explained_variance_ratio_

# Visualize PCA components
plt.scatter(pca_data[:, 0], pca_data[:, 1])
plt.title('PCA - First 2 Components')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.show()

# Output explained variance
print(f"Explained Variance Ratio of PCA Components: {explained_variance}")

"""- PCA is applied to the scaled numeric data, and the explained variance of the first two components is visualized and printed."""

from sklearn.feature_selection import SelectKBest, f_regression

# Selecting the most important features impacting 'Sales'
X = df_scaled.drop(columns=['Sales'], errors='ignore')  # Independent variables
y = df[numeric_columns]['Sales']  # Dependent variable (Sales column)

# Select top features based on f_regression
selector = SelectKBest(score_func=f_regression, k='all')  # Set 'k' to 'all' to select all available features
selector.fit(X, y)

# Output selected features
selected_columns = X.columns[selector.get_support()]
print("Selected Features Impacting Sales:", selected_columns)

"""- This section uses feature selection (SelectKBest and f_regression) to identify the most important features impacting the Sales column.

## Summary of Insights:
1. The dataset had missing values, especially marked with '?' in certain columns. We handled these by imputing median values for numeric columns and the mode for categorical columns.
2. Outliers were identified in some numeric columns using a boxplot, but no major skew was observed.
3. Scaling the numeric data was essential for proper performance of dimensionality reduction techniques and feature selection.
4. PCA revealed that the first two principal components explained a significant portion of the variance in the dataset, helping reduce dimensionality.
5. Feature selection using `SelectKBest` identified the most important features impacting `Sales`, allowing us to focus on these for further analysis or model-building.

## Challenges:
1. The presence of missing values required careful imputation techniques to avoid distorting the data.
2. Choosing the right number of PCA components required analysis of the explained variance to balance dimensionality reduction with retaining important information.
3. Ensuring that scaling and imputation steps were properly aligned to avoid data leakage was essential for a clean analysis.
"""